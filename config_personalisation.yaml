num_epochs: 8  #Number of epochs to fine tune
early_stop_checkpoint: 10
input_model_storage: "../Model Epochs/"
output_model_storage: "../Result Models/"

model_storage: "../Model Epochs/"
global_model:  "epoch_27_fold0_ccv"
    
center_fold: 1 #0 Vall 1 Sag 2 ACDC 3 San. Used when testing on whole center
num_folds: 5 #number of folds for data split

train_loss_threshold: 1.6 #Needs to be set since sometimes Validation loss is not reperesntative. E.g. When the model guesses 

data:
    centres:
        - "Vall d'Hebron" 
        - "Sagrada Familia"
        - "ACDC"
        - "SantPau" 

model:
    # The model name used here will generate a new experiment in the ~/mnm/experiments
    arch:
        function: models.nets.ResNet3D_18_Classifier
        #models.nets.ResNet18Classifier 
        dimensionality: '3D' # or 2D
        args: #keyword arguments for function
            pretrained: True
            in_ch: 3
            out_ch: 1
            linear_ch: 512
#             out_ch: 1 #
            early_layers_learning_rate: 1e-5 #10^-5, if set to 0 early layers will be frozen 
            seed: 2 # Particularly important for federated. Models will make no sense if we aggregate from different initializations

hyperparameters:
    criterion : torch.nn.BCELoss
    optimizer : torch.optim.Adam
    lr : 0.01

train_private: False #Makes the program train a fresh model on one center
save_models: False #Save the best epoch of every training session
